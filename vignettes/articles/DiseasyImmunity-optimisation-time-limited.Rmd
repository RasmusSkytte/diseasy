---
title: "DiseasyImmunity optimisation"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(diseasy)
```


```{r reset}
if (exists("im")) rm(im)
im <- DiseasyImmunity$new()

```

We define a "family" of functions to test against.
These start at 1 and go to 0 within a time scale.

We construct more target functions from this family of functions:
- The unaltered functions (f(t))
- The functions but with non-zero asymptote (g(t) = 0.8 f(t) + 0.2)
- The functions but with longer time scales (h(t) = f(t / 2))

```{r define waning functions}

# We define our list of "raw" test functions
f <- list(
  #"exponential" = \(t) exp(-t / time_scale),
  #"sigmoidal" = \(t) exp(-(t - time_scale) / 6) / (1 + exp(-(t - time_scale) / 6)),
  #"heaviside" = \(t) as.numeric(t < time_scale),
  #"exp_sum" = \(t) (exp(-t / time_scale) + exp(-2 * t / time_scale) + exp(-3 * t / time_scale)) / 3,
  "linear" = \(t) pmax(1 - t / time_scale, 0)
)
# And define a list of corresponding time_scales
t_f <- rep(20, length(f))

# Construct a list with a non-zero asymptote
g <- list(
  #"exponential" = \(t) 0.2 + 0.8 * exp(-t / time_scale),
  #"sigmoidal" = \(t) 0.2 + 0.8 * exp(-(t - time_scale) / 6) / (1 + exp(-(t - time_scale) / 6)),
  #"heaviside" = \(t) 0.2 + 0.8 * as.numeric(t < time_scale),
  #"exp_sum" = \(t) (exp(-t / time_scale) + exp(-2 * t / time_scale) + exp(-3 * t / time_scale)) / 3,
  "linear" = \(t) 0.2 + 0.8 * pmax(1 - t / time_scale, 0)
)
t_g <- t_f

# Construct a list with double the time scale
h <- f
t_h <- t_f * 2


# Construct a list of all models
#models <- c(f, g, h)
#model_names <- c(paste0(names(f), "-0"), paste0(names(f), "-c"), paste0(names(f), "-2t"))
#time_scales <- c(t_f, t_g, t_h)
models <- c(f, h)
model_names <- c(paste0(names(f), "-0"), paste0(names(f), "-2t"))
time_scales <- c(t_f, t_h)

zip <- function(...) mapply(list, ..., SIMPLIFY = FALSE)

```


Now we can run the approximation for all target functions as we increase the
number of compartments.

This utilises a caching system where simulations results are stored to file
since the computations are slow. This way we can pick up where we left of as
we run the approximations.

First, we need to determine how to optimise the parameters since quality of the
fit is seemingly very dependent on the method used
```{r optimiser helper}
time_limit <- 60

optimiser <- function(combinations, monotonous, individual_level, cache, compute_new = TRUE) {

  progressr::with_progress(
    handlers = progressr::handler_progress(
      format   = ":spin :current/:total [:bar] :percent in :elapsed ETA: :eta",
      width    = 60,
      complete = "+"
    ),
    expr = {
      p <- progressr::progressor(along = combinations)
  
      invisible(future.apply::future_lapply(
        combinations,
        future.seed = TRUE,
        FUN = \(combination) {
  
          monotonous <- monotonous
          individual_level <- individual_level
  
          # Unpack model
          model_zip <- combination[[1]][[1]]
          method <- combination[[1]][[2]]
          N <- combination[[1]][[3]]
          optim_config <- combination[[1]][[4]]
          optim_label <- combination[[1]][[5]]
  
          # Not sure where a random seed is used, but future_lapply detects a call to rng
          set.seed(N)
  
          model <- model_zip[[1]]
          model_name <- model_zip[[2]]
          time_scale <- model_zip[[3]]
  
          im_p <- DiseasyImmunity$new()
          # Set the target
          im_p$set_waning_model(model, time_scale = time_scale, target = "infection")
  
          # Generate approximations and store them and the convergence point
          key <- glue::glue("{method}-{optim_label}-{monotonous}-{individual_level}-{N}")
  
          # Get the results up until now
          current_approxes <- cache$get(key = key)
  
          # Compute next values
          if (compute_new && !(model_name %in% names(current_approxes)) && purrr::pluck(current_approxes, "execution_time") < time_limit) {
  
            try(
              {
                approx <- im_p$approximate_compartmental(
                  method = method,
                  N = N,
                  monotonous = monotonous,
                  individual_level = individual_level,
                  optim_control = optim_config
                )
      
                # Get cache again
                current_approxes <- cache$get(key = key)
      
                # Append to existing results
                if (cachem::is.key_missing(current_approxes)) {
                  current_approxes <- list("execution_time" = 0)
                }
      
                current_approxes <- modifyList(
                  current_approxes, 
                  c(
                    stats::setNames(list(approx), model_name),
                    "execution_time" = current_approxes$execution_time + approx$execution_time
                  )
                )
      
                # Store to cache
                cache$set(key = key, current_approxes)
              }
            )
          }
  
          p()
  
          rm(im_p)
        }
      ))
    }
  )
}
```


We increase N and weed out bad and slow algos iteratively. This means we may accidentally kill the "best" algo for large N at small N but we have to risk this.


```{r run different optimisation algos}
path <- devtools::package_file("vignettes/articles/article_data/optim_time_limited")
cache <- cachem::cache_disk(dir = path)

# Set the optimiser configurations to test
optim_configs <- list(
  # Optim algos
  list("method" = "Nelder-Mead"),
  list("method" = "BFGS"),
  list("method" = "CG"),
  
  # Optimx algos:
  data.frame("method" = "subplex", "maxfeval" = 1e4),
  data.frame("method" = "nlm", "maxit" = 1e4),
  data.frame("method" = "nlminb", "maxit" = 1e4, "maxfeval" = 1e4),
  data.frame("method" = "spg", "maxit" = 1e4, "maxfeval" = 1e4),
  data.frame("method" = "nvm", "maxit" = 1e4), # Breaks for N = 10?
  data.frame("method" = "slsqp", "maxeval" = 1e4),
  data.frame("method" = "tnewt", "maxeval" = 1e4),
  data.frame("method" = "anms", "maxfeval" = 1e4), # too slow!
  data.frame("method" = "pracmanm", "maxfeval" = 1e4), # too slow!
  data.frame("method" = "nlnm", "maxfeval" = 1e4),
  data.frame("method" = "ucminf", "maxit" = 1e4),
  data.frame("method" = "newuoa", "maxfeval" = 1e4),
  data.frame("method" = "bobyqa", "maxfeval" = 1e4),
  data.frame("method" = "nmkb", "maxfeval" = 1e4),

  # nloptr algos:
  list("algorithm" = "auglag"),
  list("algorithm" = "bobyqa"),
  list("algorithm" = "ccsaq"),
  list("algorithm" = "cobyla"),
  list("algorithm" = "lbfgs"),
  list("algorithm" = "mma"),
  list("algorithm" = "neldermead"),
  list("algorithm" = "newuoa"),
  list("algorithm" = "sbplx"),
  list("algorithm" = "slsqp"),
  list("algorithm" = "tnewton"),
  list("algorithm" = "varmetric")
)

# Set labels for the methods
optim_labels <- optim_configs |>
  purrr::map_chr( ~ {
    .x |>
      as.data.frame() |>
      tidyr::unite("label", tidyselect::everything()) |>
      dplyr::pull("label") |>
      paste(collapse = "_")
  }) |>
    tolower()

names(optim_configs) <- optim_labels


#future::plan("sequential")
future::plan("multisession")

for (N in seq(from = 2, to = 3)) {
  combinations <- tidyr::expand_grid(
    model = zip(models, model_names, time_scales),
    method = c("free_delta", "free_gamma", "all_free"),
    N = N,
    optim_config = optim_configs
  ) |>
    dplyr::mutate(
      "optim_label" = optim_labels
    )
  
  # Run the approximations for the round
  combinations_zip <- combinations |>
    purrr::pmap(~ zip(list(..1), ..2, ..3, list(..4), ..5))
  
  monotonous <- 0
  individual_level <- 0

  optimiser(combinations_zip, monotonous = monotonous, individual_level = individual_level, cache = cache)
  
  # Gather the results for the round and eliminate stragglers
  results <- list.files(path) |>
    purrr::map(\(file) {
    tmp <- file.path(path, file) |>
      readRDS()
    
    tmp |>
      purrr::list_transpose() |>
      tibble::as_tibble() |>
      dplyr::select(dplyr::any_of(c("method", "N", "value", "execution_time"))) |>
      dplyr::mutate(
        "optim_method" = names(!!tmp), 
        "target" = stringr::str_extract(!!file, r"{\w+-\w+}"),
        .before = dplyr::everything()
      )
    }) |>
    purrr::list_rbind() |>
    dplyr::summarise(
      "value" = sum(.data$value), 
      "execution_time" = sum(.data$execution_time), 
      .by = c("optim_method", "method")
    )
}
```

