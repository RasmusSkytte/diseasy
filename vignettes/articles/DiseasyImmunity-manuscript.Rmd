---
title: "DiseasyImmunity manuscript"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(diseasy)
```


```{r reset}
if (exists("im")) rm(im)
im <- DiseasyImmunity$new()

```

We define a "family" of functions to test against.
These start at 1 and go to 0 within a time scale.

We construct more target functions from this family of functions:
- The unaltered functions (f(t))
- The functions but with non-zero asymptote (g(t) = 0.8 f(t) + 0.2)
- The functions but with longer time scales (h(t) = f(t / 2))

```{r define waning functions}

# We define our list of "raw" test functions
f <- list(
  "exponential" = \(t) exp(-t / time_scale),
  #"sigmoidal" = \(t) exp(-(t - time_scale) / 6) / (1 + exp(-(t - time_scale) / 6)),
  #"heaviside" = \(t) as.numeric(t < time_scale),
  #"exp_sum" = \(t) (exp(-t / time_scale) + exp(-2 * t / time_scale) + exp(-3 * t / time_scale)) / 3,
  "linear" = \(t) pmax(1 - t / time_scale, 0)
)
# And define a list of corresponding time_scales
t_f <- rep(20, length(f))

# Construct a list with a non-zero asymptote
g <- list(
  "exponential" = \(t) 0.2 + 0.8 * exp(-t / time_scale),
  #"sigmoidal" = \(t) 0.2 + 0.8 * exp(-(t - time_scale) / 6) / (1 + exp(-(t - time_scale) / 6)),
  #"heaviside" = \(t) 0.2 + 0.8 * as.numeric(t < time_scale),
  #"exp_sum" = \(t) (exp(-t / time_scale) + exp(-2 * t / time_scale) + exp(-3 * t / time_scale)) / 3,
  "linear" = \(t) 0.2 + 0.8 * pmax(1 - t / time_scale, 0)
)
t_g <- t_f

# Construct a list with double the time scale
h <- f
t_h <- t_f * 2


# Construct a list of all models
#models <- c(f, g, h)
#model_names <- c(paste0(names(f), "-0"), paste0(names(f), "-c"), paste0(names(f), "-2t"))
#time_scales <- c(t_f, t_g, t_h)
models <- c(f, h)
model_names <- c(paste0(names(f), "-0"), paste0(names(f), "-2t"))
time_scales <- c(t_f, t_h)

zip <- function(...) mapply(list, ..., SIMPLIFY = FALSE)

```

We run a few examples to manually inspect the fitting
```{r manually check fits}
im$set_waning_model(f[["linear"]], time_scale = t_f[[1]], target = "infection")
plot(im, method = "free_gamma", N = 3)

im$approximate_compartmental(method = "free_delta", N = 2, optim_control = list("method" = "BFGS", "maxit" = 20))
```

```{r manually check fits}
im$set_waning_model(f[["linear"]], time_scale = t_f[[1]], target = "infection")
plot(im, method = "free_gamma", N = 3)

im$approximate_compartmental(method = "free_delta", N = 2, optim_control = list("algorithm" = "newuoa", "maxeval" = 200))
```


Now we can run the approximation for all target functions as we increase the
number of compartments.

This utilises a caching system where simulations results are stored to file
since the computations are slow. This way we can pick up where we left of as
we run the approximations.

First, we need to determine how to optimise the parameters since quality of the
fit is seemingly very dependent on the method used
```{r optimiser helper}
optimiser <- function(combinations, monotonous, individual_level, cache) {

  progressr::with_progress({
    p <- progressr::progressor(steps = length(combinations))

    invisible(future.apply::future_lapply(
      combinations,
      future.seed = TRUE,
      FUN = \(combination) {

        monotonous <- monotonous
        individual_level <- individual_level

        # Unpack model
        model_zip <- combination[[1]][[1]]
        method <- combination[[1]][[2]]
        N <- combination[[1]][[3]]
        optim_config <- combination[[1]][[4]]

        # Not sure where a random seed is used, but future_lapply detects a call to rng
        set.seed(N)

        model <- model_zip[[1]]
        model_name <- model_zip[[2]]
        time_scale <- model_zip[[3]]

        im_p <- DiseasyImmunity$new()
        # Set the target
        im_p$set_waning_model(model, time_scale = time_scale, target = "infection")

        # Generate approximations and store them and the convergence point

        # Get the results up until now
        current_approxes <- cache$get(key = glue::glue("{model_name}-{method}-{monotonous}-{individual_level}-{N}"))

        mc <- optim_config |>
          as.data.frame() |>
          tidyr::unite("label", tidyselect::everything())

        optim_label <- paste(mc$label, collapse = "_")

        # Compute next values
        if (!(optim_label %in% names(current_approxes))) {

          approx <- im_p$approximate_compartmental(
            method = method,
            N = N,
            monotonous = monotonous,
            individual_level = individual_level,
            optim_control = optim_config
          )

          # Get cache again
          current_approxes <- cache$get(key = glue::glue("{model_name}-{method}-{monotonous}-{individual_level}-{N}"))

          # Append to existing results
          if (cachem::is.key_missing(current_approxes)) {
            current_approxes <- list()
          }

          current_approxes <- modifyList(current_approxes, stats::setNames(list(approx), optim_label))

          # Store to cache
          cache$set(key = glue::glue("{model_name}-{method}-{monotonous}-{individual_level}-{N}"), current_approxes)
        }

        p()

        rm(im_p)
      }
    ))
  })
}
```


```{r summariser helper}
summariser <- function(combinations) {
  optim_summery <- combinations |>
    dplyr::select(!"optim_config") |>
    dplyr::distinct() |>
    purrr::pmap(
      .progress = interactive(),
      .f = \(model_zip, method, N, optim_config) {

        # Unpack model
        model <- model_zip[[1]]
        model_name <- model_zip[[2]]
        time_scale <- model_zip[[3]]

        # Check if we have already computed this value
        current_approxes <- cache$get(key = glue::glue("{model_name}-{method}-{monotonous}-{individual_level}-{N}"))

        if (cachem::is.key_missing(current_approxes)) {

          return(NULL)

        } else {
          summery <- current_approxes |>
            purrr::list_transpose() |>
            tibble::as_tibble() |>
            dplyr::transmute(
              "optim_method" = names(current_approxes),
              "model" = !!model_name,
              .data$method,
              .data$N,
              .data$convergence,
              .data$value,
              .data$execution_time
            )

          return(summery)
        }
      }
    ) |>
    purrr::list_rbind()

  # First overview
  optim_summery |>
    dplyr::select(!"execution_time") |>
    tidyr::pivot_wider(
      id_cols = c("optim_method", "model", "N"),
      names_from = "method",
      values_from = c("value", "convergence"),
      names_glue = "{method}_{.value}",
    ) |>
    dplyr::select("optim_method", "model", "N", starts_with("free_delta"), starts_with("free_gamma"), starts_with("all_free")) |>
    dplyr::arrange(.data$free_delta_value + .data$free_gamma_value + .data$all_free_value) |>
    print()

  # Single metric overview
  tt <- optim_summery |>
    dplyr::summarise(
      "value" = sum(.data$value),
      "execution_time" = sum(.data$execution_time),
      "convergence" = mean(.data$convergence),
      .by = c("optim_method", "N")
    ) |>
    dplyr::arrange(.data$value) |>
    dplyr::filter(is.finite(.data$value))

  print(tt)

  # Convergence
  optim_summery |>
    dplyr::summarise(max_convergence = max(.data$convergence), .by = c("optim_method", "N")) |>
    dplyr::filter(max_convergence == 0)

  # Plot points with names of method
  tt |>
    dplyr::filter(is.finite(.data$value)) |>
    #dplyr::filter(.data$value <= 2*median(.data$value), .data$execution_time <= 2*median(.data$execution_time), .by =  "N") |>
    ggplot2::ggplot(ggplot2::aes(x = value, y = as.numeric(execution_time), color = optim_method, shape = cut(convergence, breaks = c(0, 0.25, 0.5, 0.75, 1, Inf), right = TRUE, include.lowest = TRUE))) +
    ggplot2::geom_point() +
    ggplot2::geom_text(ggplot2::aes(label = optim_method), nudge_x = 0.5, nudge_y = 0, vjust = 0.5, hjust = 0) +
    ggplot2::labs(x = "Value", y = "Execution time", shape = "Convergence", color = "Method") +
    ggplot2::theme_bw() +
    ggplot2::xlim(0, 10) +
    ggplot2::ylim(0, NA) +
    ggplot2::facet_wrap(~ N, scales = "free_y")

  # # Plot points with names of method
  # optim_summery |>
  #   dplyr::summarise(
  #     "value" = sum(.data$value),
  #     "execution_time" = sum(.data$execution_time),
  #     "convergence" = mean(.data$convergence),
  #     .by = c("optim_method" ,"method", "N")
  #   ) |>
  #   dplyr::filter(is.finite(.data$value)) |>
  #   dplyr::filter(.data$value <= mean(.data$value), .data$execution_time <= mean(.data$execution_time), .by =  c("method", "N")) |>
  #   ggplot2::ggplot(ggplot2::aes(x = value, y = as.numeric(execution_time), color = optim_method, shape = cut(convergence, breaks = c(0, 0.25, 0.5, 0.75, 1), right = TRUE, include.lowest = TRUE))) +
  #   ggplot2::geom_point() +
  #   ggplot2::geom_text(ggplot2::aes(label = optim_method), nudge_x = 0.5, nudge_y = 0, vjust = 0.5, hjust = 0) +
  #   ggplot2::labs(x = "Value", y = "Execution time", shape = "Convergence", color = "Method") +
  #   ggplot2::theme_bw() +
  #   ggplot2::xlim(0, 10) +
  #   ggplot2::ylim(0, NA) +
  #   ggplot2::facet_grid(N ~ method, scales = "free_y")
}
```



```{r run different optimisation algos}
cache <- cachem::cache_disk(dir = devtools::package_file("vignettes/articles/article_data/optim_nloptr"))

# Set the optimiser configurations to test
optim_configs <- list(
  # Optim algos
  list("method" = "Nelder-Mead", "maxit" = 10),
  list("method" = "BFGS", "maxit" = 10),
  list("method" = "CG", "maxit" = 10),

  # Optimx algos:
  data.frame("method" = "subplex", "maxfeval" = 200),
  data.frame("method" = "nlm", "maxit" = 10),
  data.frame("method" = "nlminb", "maxit" = 10, "maxfeval" = 200),
  #data.frame("method" = "Rcgmin", "maxit" = 10, "maxfeval" = 200), # Method issue, NA params?
  #data.frame("method" = "Rtnmin", "maxit" = 10, "maxfeval" = 200), # Method issue, NA params?
  #data.frame("method" = "Rvmmin", "maxit" = 10, "maxfeval" = 200), # Method issue, NA params?
  #data.frame("method" = "snewton", "maxit" = 10, "maxfeval" = 200), # Needs gradient and Hessian
  #data.frame("method" = "snewtonm", "maxit" = 10, "maxfeval" = 200), # Needs gradient and Hessian
  data.frame("method" = "spg", "maxit" = 10, "maxfeval" = 200),
  data.frame("method" = "nvm", "maxit" = 10),
  data.frame("method" = "slsqp", "maxeval" = 200),
  data.frame("method" = "tnewt", "maxeval" = 200),
  #data.frame("method" = "anms", "maxfeval" = 200), # Very slow
  data.frame("method" = "pracmanm", "maxfeval" = 200),
  data.frame("method" = "nlnm", "maxfeval" = 200),
  data.frame("method" = "ucminf", "maxit" = 10),
  data.frame("method" = "newuoa", "maxfeval" = 200),
  data.frame("method" = "bobyqa", "maxfeval" = 200),
  data.frame("method" = "nmkb", "maxfeval" = 200),
  # data.frame("method" = "hjkb", "maxfeval" = 200), # Function evaluation limit exceeded -- may not converge.
  # data.frame("method" = "lbfgsb3c", "maxit" = 10)#,
  # #data.frame("method" = "lbfgs"), # Needs params?
  # #data.frame("method" = "mla"), # Needs params?
  # #data.frame("method" = "snewtm", "maxit" = 10, "maxfeval" = 200), # Needs gradient and Hessian

  # nloptr algos:
  list("algorithm" = "auglag", "maxeval" = 200),
  list("algorithm" = "auglag", "maxeval" = 400),
  list("algorithm" = "bobyqa", "maxeval" = 200),
  list("algorithm" = "bobyqa", "maxeval" = 400),
  list("algorithm" = "ccsaq", "maxeval" = 200),
  list("algorithm" = "cobyla", "maxeval" = 200),
  #list("algorithm" = "crs2lm", "maxeval" = 200), Requires lower, upper
  #list("algorithm" = "direct", "maxeval" = 200), Requires lower, upper
  #list("algorithm" = "directL", "maxeval" = 200), Requires lower, upper
  #list("algorithm" = "isres", "maxeval" = 200), Requires lower, upper
  list("algorithm" = "lbfgs", "maxeval" = 200),
  #list("algorithm" = "mlsl", "maxeval" = 200), Requires lower, upper
  list("algorithm" = "mma", "maxeval" = 200),
  list("algorithm" = "neldermead", "maxeval" = 200),
  list("algorithm" = "newuoa", "maxeval" = 200),
  list("algorithm" = "newuoa", "maxeval" = 400),
  list("algorithm" = "sbplx", "maxeval" = 200),
  list("algorithm" = "slsqp", "maxeval" = 200),
  #list("algorithm" = "stogo", "maxeval" = 200), Has no control args
  list("algorithm" = "tnewton", "maxeval" = 200),
  list("algorithm" = "varmetric", "maxeval" = 200)
)


# Set the combinations to test
combinations <- tidyr::expand_grid(
  model = zip(models, model_names, time_scales),
  method = c("free_gamma", "free_delta", "all_free"),
  N = c(3, 5),
  optim_config = optim_configs
)

#future::plan("sequential")
future::plan("multisession", workers = 6)


# Run the approximations and note the convergence point
combinations_zip <- combinations |>
  purrr::pmap(~ zip(list(..1), ..2, ..3, list(..4)))



optimiser(combinations_zip, monotonous = 100, individual_level = 0, cache = cache)

summariser(combinations)
```


```{r run different optimisation algos}
cache <- cachem::cache_disk(dir = devtools::package_file("vignettes/articles/article_data/optim_nloptr"))

# Set the optimiser configurations to test
optim_configs <- list(
  # Optimx algos:
  data.frame("method" = "Nelder-Mead", "maxit" = 10),
  data.frame("method" = "BFGS", "maxit" = 10),
  data.frame("method" = "CG", "maxit" = 10),
  data.frame("method" = "subplex", "maxfeval" = 200),
  data.frame("method" = "nlm", "maxit" = 10),
  data.frame("method" = "nlminb", "maxit" = 10, "maxfeval" = 200),
  #data.frame("method" = "Rcgmin", "maxit" = 10, "maxfeval" = 200), # Method issue, NA params?
  #data.frame("method" = "Rtnmin", "maxit" = 10, "maxfeval" = 200), # Method issue, NA params?
  #data.frame("method" = "Rvmmin", "maxit" = 10, "maxfeval" = 200), # Method issue, NA params?
  #data.frame("method" = "snewton", "maxit" = 10, "maxfeval" = 200), # Needs gradient and Hessian
  #data.frame("method" = "snewtonm", "maxit" = 10, "maxfeval" = 200), # Needs gradient and Hessian
  data.frame("method" = "spg", "maxit" = 10, "maxfeval" = 200),
  data.frame("method" = "nvm", "maxit" = 10),
  data.frame("method" = "slsqp", "maxeval" = 200),
  data.frame("method" = "tnewt", "maxeval" = 200),
  #data.frame("method" = "anms", "maxfeval" = 200), # Very slow
  data.frame("method" = "pracmanm", "maxfeval" = 200),
  data.frame("method" = "nlnm", "maxfeval" = 200),
  data.frame("method" = "ucminf", "maxit" = 10),
  data.frame("method" = "newuoa", "maxfeval" = 200),
  data.frame("method" = "bobyqa", "maxfeval" = 200),
  data.frame("method" = "nmkb", "maxfeval" = 200),
  # data.frame("method" = "hjkb", "maxfeval" = 200), # Function evaluation limit exceeded -- may not converge.
  # data.frame("method" = "lbfgsb3c", "maxit" = 10)#,
  # #data.frame("method" = "lbfgs"), # Needs params?
  # #data.frame("method" = "mla"), # Needs params?
  # #data.frame("method" = "snewtm", "maxit" = 10, "maxfeval" = 200), # Needs gradient and Hessian

  # nloptr algos:
  list("algorithm" = "auglag", "maxeval" = 200),
  list("algorithm" = "auglag", "maxeval" = 400),
  list("algorithm" = "bobyqa", "maxeval" = 200),
  list("algorithm" = "bobyqa", "maxeval" = 400),
  list("algorithm" = "ccsaq", "maxeval" = 200),
  list("algorithm" = "cobyla", "maxeval" = 200),
  #list("algorithm" = "crs2lm", "maxeval" = 200), Requires lower, upper
  #list("algorithm" = "direct", "maxeval" = 200), Requires lower, upper
  #list("algorithm" = "directL", "maxeval" = 200), Requires lower, upper
  #list("algorithm" = "isres", "maxeval" = 200), Requires lower, upper
  list("algorithm" = "lbfgs", "maxeval" = 200),
  #list("algorithm" = "mlsl", "maxeval" = 200), Requires lower, upper
  list("algorithm" = "mma", "maxeval" = 200),
  list("algorithm" = "neldermead", "maxeval" = 200),
  list("algorithm" = "newuoa", "maxeval" = 200),
  list("algorithm" = "newuoa", "maxeval" = 400),
  list("algorithm" = "sbplx", "maxeval" = 200),
  list("algorithm" = "slsqp", "maxeval" = 200),
  #list("algorithm" = "stogo", "maxeval" = 200), Has no control args
  list("algorithm" = "tnewton", "maxeval" = 200),
  list("algorithm" = "varmetric", "maxeval" = 200)
)


# Set the combinations to test
combinations <- tidyr::expand_grid(
  model = zip(models, model_names, time_scales),
  method = c("free_gamma", "free_delta", "all_free"),
  N = c(3, 5),
  optim_config = optim_configs
)

#future::plan("sequential")
future::plan("multisession", workers = 6)


# Run the approximations and note the convergence point
combinations_zip <- combinations |>
  purrr::pmap(~ zip(list(..1), ..2, ..3, list(..4)))



optimiser(combinations_zip, monotonous = 100, individual_level = 0, cache = cache)

summariser(combinations)
```

# nlm looks fast, can we do better?

```{r run different optimisation algos nlm speed}
cache <- cachem::cache_disk(dir = devtools::package_file("vignettes/articles/article_data/optim_nlm"))

# Set the optimiser configurations to test
optim_configs <- list(
  data.frame("method" = "nlm", "maxit" = 5),
  data.frame("method" = "nlm", "maxit" = 10),
  data.frame("method" = "nlm", "maxit" = 20),
  data.frame("method" = "nlm", "maxit" = 40),
  data.frame("method" = "nlnm", "maxfeval" = 50),
  data.frame("method" = "nlnm", "maxfeval" = 100),
  data.frame("method" = "nlnm", "maxfeval" = 200),
  data.frame("method" = "nlnm", "maxfeval" = 300),
  data.frame("method" = "nlnm", "maxfeval" = 400),
  data.frame("method" = "nlminb", "maxit" = 10, "maxfeval" = 100),
  data.frame("method" = "nlminb", "maxit" = 5, "maxfeval" = 200),
  data.frame("method" = "nlminb", "maxit" = 10, "maxfeval" = 200),
  data.frame("method" = "BFGS", "maxit" = 10),
  data.frame("method" = "BFGS", "maxit" = 25),
  data.frame("method" = "BFGS", "maxit" = 50),
  data.frame("method" = "BFGS", "maxit" = 100)
)


# Set the combinations to test
combinations <- tidyr::expand_grid(
  model = zip(models, model_names, time_scales),
  method = c("free_gamma", "free_delta", "all_free"),
  N = c(3, 5),
  optim_config = optim_configs
)

future::plan("sequential")
#future::plan("multisession", workers = 6)


# Run the approximations and note the convergence point
combinations_zip <- combinations |>
  purrr::pmap(~ zip(list(..1), ..2, ..3, list(..4)))


optimiser(combinations_zip, monotonous = 100, individual_level = 0, cache = cache)

summariser(combinations)
```


```{r run different optimisation algos nlm speed}
cache <- cachem::cache_disk(dir = devtools::package_file("vignettes/articles/article_data/optim_nlm_combi"))

# Set the optimiser configurations to test
optim_configs <- list(
  data.frame("method" = c("nlm", "BFGS"), "maxit" = c(5, 25)),
  data.frame("method" = c("nlm", "BFGS"), "maxit" = c(10, 25)),
  data.frame("method" = c("nlm", "BFGS"), "maxit" = c(5, 50)),
  data.frame("method" = c("nlm", "nlnm"), "maxit" = 5, "maxfeval" = 100),
  data.frame("method" = c("nlm", "nlnm"), "maxit" = 10, "maxfeval" = 100),
  data.frame("method" = c("nlm", "nlnm"), "maxit" = 10, "maxfeval" = 200),
  data.frame("method" = "nlnm", "maxfeval" = 50),
  data.frame("method" = "nlnm", "maxfeval" = 100),
  data.frame("method" = "nlnm", "maxfeval" = 200)
)

# Set the combinations to test
combinations <- tidyr::expand_grid(
  model = zip(models, model_names, time_scales),
  method = c("free_gamma", "free_delta", "all_free"),
  N = c(3, 5, 10),
  optim_config = optim_configs
)

future::plan("sequential")
#future::plan("multisession", workers = 6)


# Run the approximations and note the convergence point
combinations_zip <- combinations |>
  purrr::pmap(~ zip(list(..1), ..2, ..3, list(..4)))

optimiser(combinations_zip, monotonous = 100, individual_level = 0, cache = cache)

summariser(combinations)
```




```{r run all single targets}
cache <- cachem::cache_disk(dir = devtools::package_file("vignettes/articles/article_data/single"))

N_max <- 5                                                                                                            # nolint: object_name_linter

# Set the combinations to test
combinations_single <- tidyr::expand_grid(
  model = zip(models, model_names, time_scales),
  method = c("free_gamma", "free_delta", "all_free"),
  N = seq(from = 2, to = N_max, by = 1)
)

#future::plan("sequential")
future::plan("multisession", workers = 15)

# Run the approximations and note the convergence point
combinations <- combinations_single |>
  purrr::pmap(~ zip(list(..1), ..2, ..3))

monotonous <- 100
individual_level <- 0

progressr::with_progress({
  p <- progressr::progressor(steps = length(combinations))

  invisible(future.apply::future_lapply(
    combinations,
    future.seed = TRUE,
    FUN = \(combination) {

      monotonous <- monotonous
      individual_level <- individual_level

      # Unpack model
      model_zip <- combination[[1]][[1]]
      method <- combination[[1]][[2]]
      N <- combination[[1]][[3]]

      # Not sure where a random seed is used, but future_lapply detects a call to rng
      set.seed(N)

      model <- model_zip[[1]]
      model_name <- model_zip[[2]]
      time_scale <- model_zip[[3]]

      im_p <- DiseasyImmunity$new()
      # Set the target
      im_p$set_waning_model(model, time_scale = time_scale, target = "infection")

      # Generate approximations and store them and the convergence point

      # Get the results up until now
      current_approxes <- cache$get(key = glue::glue("{model_name}-{method}-{monotonous}-{individual_level}"))

      # Compute next values
      if (!(N %in% names(current_approxes))) {

        approx <- im_p$approximate_compartmental(method = method, N = N, monotonous = monotonous, individual_level = individual_level, optim.method = c(rep("subplex", 2), "CG"), maxit = c(rep(100, 2), 100), maxfeval = c(rep(250, 2), 1000))

        # Get cache again
        current_approxes <- cache$get(key = glue::glue("{model_name}-{method}-{monotonous}-{individual_level}"))

        # Append to existing results
        if (cachem::is.key_missing(current_approxes)) {
          current_approxes <- list()
        }

        current_approxes <- modifyList(current_approxes, stats::setNames(list(approx), N))

        # Store to cache
        cache$set(key = glue::glue("{model_name}-{method}-{monotonous}-{individual_level}"), current_approxes)
      }

      p()

      rm(im_p)
    }
  ))
})
```

The solutions have a few "issues".
- They are not always monotonous
- They are not always converged as the problem can be degenerate

The latter is not necessarily a problem since we mostly care about having the
best approximation.

Adding a penalty for spread of gamma values does not make more solutions converge.

Also note that I have tried changing the optimiser method without any noticeable
difference in the results.

```{r check monotonousity in solution}
solution_summery_single <- combinations_single |>
  purrr::pmap(
    .progress = interactive(),
    .f = \(model_zip, method, N) {

      # Unpack model
      model <- model_zip[[1]]
      model_name <- model_zip[[2]]
      time_scale <- model_zip[[3]]

      # Check if we have already computed this value
      current_approxes <- cache$get(key = glue::glue("{model_name}-{method}-{monotonous}-{individual_level}"))
      if (cachem::is.key_missing(current_approxes)) {
        return(NULL)
      } else {
        monotoncity <- current_approxes |>
          purrr::list_transpose() |>
          tibble::as_tibble() |>
          dplyr::transmute(
            "model" = !!model_name,
            .data$method,
            .data$N,
            .data$gamma,
            .data$convergence,
            "monotone" = purrr::map_lgl(.data$gamma, ~ all(diff(.) <= 0))
          )
        return(monotoncity)
      }
    }
  ) |>
  purrr::list_rbind()

solution_summery_single |>
  dplyr::filter(!.data$monotone)


solution_summery_single |>
  dplyr::filter(.data$convergence != 0)
```

```{r}
solution_summery_single |>
  dplyr::filter(.data$convergence != 0) |>
  head(1) |>
  dplyr::pull("gamma")
```


```{r detect single target convergence}
convergence_single <- combinations_single |>
  purrr::pmap_dbl(
    .progress = interactive(),
    .f = \(model_zip, method, N) {

      # Unpack model
      model <- model_zip[[1]]
      model_name <- model_zip[[2]]
      time_scale <- model_zip[[3]]

      # Get the results
      current_approxes <- cache$get(key = glue::glue("{model_name}-{method}-{monotonous}-{individual_level}"))

      # Check if approximation has converged
      prev_values <- purrr::map_dbl(current_approxes, ~ purrr::pluck(., "value")) |>
        tibble::enframe(name = "N", value = "value") |>
        dplyr::mutate("N" = as.numeric(N))

      # Look for the N that caused convergence using Kneedle algorithm
      N_converged <- tryCatch(                                                                                          # nolint: object_name_linter
        kneedle::kneedle(
          x = prev_values$N,
          y = prev_values$value,
          decreasing = TRUE,
          concave = FALSE,
          sensitivity = 1
        ),
        error = function(e) numeric(0)
      ) |>
        purrr::pluck(1, .default = numeric(0))

      # In some cases, values are increasing not decreasing, so we manually detect these
      # (We allow a small increase to account for numerical imprecision)
      detect_increase <- \(x) (x - cummin(x)) > 1e-3 * cummin(x)
      if (identical(N_converged, numeric(0)) && any(detect_increase(prev_values$value))) {
        N_converged <- prev_values$N[[which(detect_increase(prev_values$value))[1]]]                                    # nolint: object_name_linter
      }

      return(ifelse(identical(N_converged, numeric(0)), NA, N_converged))
    }
  ) |>
  tibble::tibble(
    "model" = purrr::map_chr(combinations_single$model, ~ purrr::pluck(., 2)),
    "method" = combinations_single$method,
    "N" = _
  )
```

```{r elbow curve single target}
metrics_single <- combinations_single |>
  purrr::pmap(
    .progress = interactive(),
    .f = \(model_zip, method, N) {

      # Unpack model
      model <- model_zip[[1]]
      model_name <- model_zip[[2]]
      time_scale <- model_zip[[3]]

      # Check if we have already computed this value
      current_approxes <- cache$get(key = glue::glue("{model_name}-{method}-{monotonous}-{individual_level}"))
      if (cachem::is.key_missing(current_approxes)) {
        metrics <- data.frame(
          "model" = model_name,
          "method" = method,
          "N" = NA,
          "value" = NA,
          "execution_time" = NA
        )
      } else {
        metrics <- current_approxes |>
          purrr::list_transpose() |>
          tibble::as_tibble() |>
          dplyr::transmute(
            "model" = !!model_name,
            "method" = !!method,
            .data$N,
            "value" = .data$sqrt_integral,
            .data$execution_time)
      }

      return(metrics)
    }
  ) |>
  purrr::list_rbind()


# Prepare for plotting
metrics_single_gg <- metrics_single |>
  dplyr::mutate("execution_time" = as.numeric(.data$execution_time)) |>
  tidyr::separate_wider_delim("model", delim = "-", names = c("model", "variant"), too_few = "align_start") |>
  dplyr::mutate("variant" = dplyr::case_when(
    variant == "0" ~ "base",
    variant == "c" ~ "non-zero asymptote",
    variant == "2t" ~ "twice the time scale"
  )) |>
  dplyr::filter(!is.na(value)) |>
  tidyr::pivot_longer(!c("model", "variant", "method", "N"), names_to = "metric")

convergence_single_gg <- convergence_single |>
  tidyr::separate_wider_delim("model", delim = "-", names = c("model", "variant"), too_few = "align_start") |>
  dplyr::mutate(
    "variant" = dplyr::case_when(
      variant == "0" ~ "base",
      variant == "c" ~ "non-zero asymptote",
      variant == "2t" ~ "twice the time scale"
    )
  )

# Plot
ggplot2::ggplot(metrics_single_gg, ggplot2::aes(x = N, y = value, color = method, linetype = variant)) +
  ggplot2::geom_line(linewidth = 1) +
  ggplot2::geom_point() +
  ggplot2::scale_y_log10() +
  ggplot2::facet_grid(metric ~ model, scales = "free") +
  ggplot2::labs(x = "N", y = "Metric") +
  ggplot2::theme_bw()

#ggplot2::ggsave("subplex_10_subplex_10_individual_level_time.png")
```

```{r}
metrics_single_gg |>
  dplyr::filter(.data$metric == "value") |>
  ggplot2::ggplot(ggplot2::aes(x = N, y = value, color = method)) +
  ggplot2::geom_line(linewidth = 1) +
  ggplot2::geom_point() +
  ggplot2::geom_vline(data = convergence_single_gg, ggplot2::aes(xintercept = N, linetype = method, color = method)) +
  ggplot2::facet_grid(model ~ variant, scales = "free") +
  ggplot2::labs(x = "N", y = "Value") +
  ggplot2::theme_bw()

#ggplot2::ggsave(glue::glue("monotonous_{monotonous}_individual_level_{individual_level}.png"))
#ggplot2::ggsave("subplex_10_subplex_10_individual_level.png")
```

Lets investigate a few "problem" areas in the above graph
- linear, base, N = 4, free_gamma vs all_free
- linear, twice the time scale, N = 8, free_gamma vs all_free
- sigmoidal, base, N = 6, free_gamma vs all_free
- sigmoidal, twice the time scale, N = 7:10, free_gamma vs all_free

```{r debug}
purrr::pluck(cache$get(key = glue::glue("sigmoidal-free_delta")), "3")
```

```{r debug}
metrics_single |>
  dplyr::filter(.data$model == "sigmoidal", .data$method == "free_delta")
```

```{r debug}
metrics_single_gg |>
  dplyr::filter(.data$model == "sigmoidal", .data$variant == "base", .data$method == "free_delta")
```


```{r manually check fits}
im$set_waning_model(f[["sigmoidal"]], time_scale = t_f[[1]], target = "infection")
im$set_waning_model(f[["sigmoidal"]], time_scale = t_f[[1]], target = "hospitalisation")
plot(im, method = "free_delta", N = 3)

im$approximate_compartmental(method = "free_delta", N = 3)
```
```{r manually check fits}
im$set_waning_model(h[["sigmoidal"]], time_scale = t_h[[1]], target = "infection")
im$set_waning_model(h[["sigmoidal"]], time_scale = t_h[[1]], target = "hospitalisation")
plot(im, method = "free_delta", N = 3)

im$approximate_compartmental(method = "free_delta", N = 3)
```


```{r run all double targets}
cache <- cachem::cache_disk(dir = devtools::package_file("vignettes/articles/article_data/double"))

N_max <- 10

# Set the combinations to test
combinations_double <- tidyr::expand_grid(
  model1 = zip(models, model_names, time_scales),
  model2 = zip(models, model_names, time_scales),
  method = c("free_gamma", "free_delta", "all_free"),
  N = seq(from = 2, to = N_max, by = 1)
)

#future::plan("sequential")
future::plan("multisession", workers = 4)

# Run the approximations and note the convergence point
combinations <- combinations_double |>
  purrr::pmap(~ zip(list(..1), list(..2), ..3, ..4))
                                                                                                          # nolint: object_name_linter

monotonous <- 100
individual_level <- 1

progressr::with_progress({
  p <- progressr::progressor(steps = length(combinations))

  invisible(future.apply::future_lapply(
    combinations,
    future.seed = TRUE,
    FUN = \(combination) {

      monotonous <- monotonous
      individual_level <- individual_level

      # Unpack model
      model1_zip <- combination[[1]][[1]]
      model2_zip <- combination[[1]][[2]]
      method <- combination[[1]][[3]]
      N <- combination[[1]][[4]]

      model1 <- model1_zip[[1]]
      model1_name <- model1_zip[[2]]
      time_scale1 <- model1_zip[[3]]

      model2 <- model2_zip[[1]]
      model2_name <- model2_zip[[2]]
      time_scale2 <- model2_zip[[3]]

      im_p <- DiseasyImmunity$new()

      # Not sure where a random seed is used, but future_lapply detects a call to rng
      set.seed(N)

      # Get the results up until now
      current_approxes <- cache$get(key = glue::glue("{model1_name}-{model2_name}-{method}-{monotonous}-{individual_level}"))

      # Compute next values
      if (!(N %in% names(current_approxes))) {

        # Set the target and approximate
        im_p$set_waning_model(model1, time_scale = time_scale1, target = "infection")
        im_p$set_waning_model(model2, time_scale = time_scale2, target = "hospitalisation")
        approx <- im_p$approximate_compartmental(method = method, N = N, montonic = monotonous, individual_level = individual_level)

        # Get cache again
        current_approxes <- cache$get(key = glue::glue("{model1_name}-{model2_name}-{method}-{monotonous}-{individual_level}"))

        # Append to existing results
        if (cachem::is.key_missing(current_approxes)) {
          current_approxes <- list()
        }
        current_approxes <- modifyList(current_approxes, stats::setNames(list(approx), N))

        # Store to cache
        cache$set(key = glue::glue("{model1_name}-{model2_name}-{method}-{monotonous}-{individual_level}"), current_approxes)
      }

      p()

      rm(im_p)
    }
  ))
})
```

```{r detect double target convergence}
convergence_double <- combinations_double |>
  purrr::pmap(
    .progress = interactive(),
    .f = \(model1_zip, model2_zip, method, N) {

      # Unpack model
      model1 <- model1_zip[[1]]
      model1_name <- model1_zip[[2]]
      time_scale1 <- model1_zip[[3]]

      model2 <- model2_zip[[1]]
      model2_name <- model2_zip[[2]]
      time_scale2 <- model2_zip[[3]]

      # Get the results
      current_approxes <- cache$get(key = glue::glue("{model1_name}-{model2_name}-{method}-{monotonous}-{individual_level}"))


      # Check if approximation has converged
      prev_values <- purrr::map_dbl(current_approxes, ~ purrr::pluck(., "value")) |>
        tibble::enframe(name = "N", value = "value") |>
        dplyr::mutate("N" = as.numeric(N))

      # Look for the N that caused convergence using kneedle algorithm
      N_converged <- tryCatch(                                                                                          # nolint: object_name_linter
        kneedle::kneedle(
          x = prev_values$N,
          y = prev_values$value,
          decreasing = TRUE,
          concave = FALSE,
          sensitivity = 1
        ),
        error = function(e) numeric(0)
      ) |>
        purrr::pluck(1, .default = numeric(0))

      # In some cases, values are increasing not decreasing, so we manually detect these
      # (We allow a small increase to account for numerical imprecision)
      detect_increase <- \(x) (x - cummin(x)) > 1e-3 *  cummin(x)
      if (identical(N_converged, numeric(0)) && any(detect_increase(prev_values$value))) {
        N_converged <- prev_values$N[[which(detect_increase(prev_values$value))[1]]]                                    # nolint: object_name_linter
      }

      return(ifelse(identical(N_converged, numeric(0)), NA, N_converged))
    }
  ) |>
  tibble::tibble(
    "model_1" = purrr::map_chr(combinations_double$model1, ~ purrr::pluck(., 2)),
    "model_2" = purrr::map_chr(combinations_double$model2, ~ purrr::pluck(., 2)),
    "method" = combinations_double$method,
    "N" = _
  )
```

```{r elbow curve double target}
metrics_double <- combinations_double |>
  purrr::pmap(
    .progress = interactive(),
    .f = \(model1_zip, model2_zip, method, N) {

      # Unpack model
      model1 <- model1_zip[[1]]
      model1_name <- model1_zip[[2]]
      time_scale1 <- model1_zip[[3]]

      model2 <- model2_zip[[1]]
      model2_name <- model2_zip[[2]]
      time_scale2 <- model2_zip[[3]]

      # Check if we have already computed this value
      current_approxes <- cache$get(key = glue::glue("{model1_name}-{model2_name}-{method}-{monotonous}-{individual_level}"))
      if (cachem::is.key_missing(approx)) {
        metrics <- data.frame(
          "model" = !!paste(model1_name, model2_name, sep = "/"),                                                       # nolint: paste_linter
          "method" = method,
          "N" = NA,
          "value" = NA,
          "execution_time" = NA
        )
      } else {
        metrics <- current_approxes |>
          purrr::list_transpose() |>
          tibble::as_tibble() |>
          dplyr::transmute(
            "model" = !!paste(model1_name, model2_name, sep = "/"),                                                     # nolint: paste_linter
            "method" = !!method,
            .data$N,
            "value" = .data$sqrt_integral,
            .data$execution_time)
      }

      return(metrics)
    }
  ) |>
  purrr::list_rbind()


# Prepare for plotting
metrics_double_gg <- metrics_double |>
  dplyr::inner_join(
    data.frame(
      "model" = c(
        #"exp_sum-0/exp_sum-0",
        "sigmoidal-0/sigmoidal-0",
        #"exp_sum-0/sigmoidal-0",
        "sigmoidal-2t/sigmoidal-2t"#,
        #"exp_sum-0/exp_sum-2t"
      )
    ),
    by = "model"
  ) |>
  dplyr::mutate("execution_time" = as.numeric(.data$execution_time)) |>
  dplyr::filter(!is.na(value)) |>
  tidyr::pivot_longer(!c("model", "method", "N"), names_to = "metric")

convergence_double_gg <- convergence_double |>
  tidyr::unite("model", c("model_1", "model_2"), sep = "/")


# Plot
ggplot2::ggplot(metrics_double_gg, ggplot2::aes(x = N, y = value, color = method)) +
  ggplot2::geom_line(linewidth = 1) +
  ggplot2::geom_point() +
  #ggplot2::scale_y_log10() +
  ggplot2::facet_grid(metric ~ model, scales = "free") +
  ggplot2::labs(x = "N", y = "Metric") +
  ggplot2::theme_bw()
```



```{r}
metrics_double_gg <- metrics_double |>
  dplyr::filter(N == 10) |>
  tidyr::separate_wider_delim("model", delim = "/", names = c("target_1", "target_2")) |>
  dplyr::mutate(
    "target_1" = factor(.data$target_1, levels = model_names),
    "target_2" = factor(.data$target_2, levels = model_names)
  )

metrics_double_gg |>
  ggplot2::ggplot(ggplot2::aes(x = target_1, y = target_2, fill = value)) +
  ggplot2::geom_tile() +
  ggplot2::facet_wrap(~ method) +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 90, vjust = 0.5))

grid::grid.text("f", x = grid::unit(0.91, "npc"), y = grid::unit(0.80, "npc"))
```
